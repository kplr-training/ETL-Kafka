
### Read File Data with Connect

To startup a FileStream Source Connector that reads structured data from a file and exports the data into Kafka, using Schema Registry to inform Connect of their structure, we will use one of the supported connector configurations that come pre-defined with Confluent CLI confluent local commands. To get the list of all the pre-defined connector configurations, run:

confluent local services connect connector list

Bundled Predefined Connectors (edit configuration under etc/):
   file-source
   file-sink
   replicator
You may see additional connectors listed if you have previously installed connectors.

The pre-configured connector we will use first is called file-source and its configuration file is located at ./etc/kafka/connect-file-source.properties. Below is an explanation of the contents:

```
# User defined connector instance name.
name=file-source
# The class implementing the connector
connector.class=FileStreamSource
# Maximum number of tasks to run for this connector instance
tasks.max=1
# The input file (path relative to worker's working directory)
# This is the only setting specific to the FileStreamSource
file=test.txt
# The output topic in Kafka
topic=connect-test
```

If choosing to use this tutorial without Schema Registry, you must also specify the key.converter and value.converter properties to use org.apache.kafka.connect.json.JsonConverter. This will override the converters’ settings for this connector only.

We are now ready to load the connector, but before we do that, let’s seed the file with some sample data. Note that the connector configuration specifies a relative path for the file, so you should create the file in the same directory that you will run the Kafka Connect worker from.

```
for i in {1..30}; do echo "log line $i"; done > test.txt
```

Next, start an instance of the FileStreamSourceConnector using the configuration file you defined above. You can easily do this from the command line using the Confluent CLI confluent local commands as follows:

```
  confluent local services connect connector load file-source

{
  "name": "file-source",
  "config": {
    "connector.class": "FileStreamSource",
    "tasks.max": "1",
    "file": "test.txt",
    "topics": "connect-test",
    "name": "file-source"
  },
  "tasks": []
}
```
Upon success it will print a snapshot of the connector’s configuration. To confirm which connectors are loaded any time, run:
```
  confluent local services connect connector status connectors

[
  "file-source"
]
```
With the Confluent Platform running and the CLI installed, you can find the path to the Kafka Connect log file at: $(confluent local current)/connect/connect.stdout. For example, to search the file for errors you can run: cat $(confluent local current)/connect/connect.stdout | grep ERROR

You will get a list of all the loaded connectors in this worker. The same command supplied with the connector name will give you the status of this connector, including an indication of whether the connector has started successfully or has encountered a failure. For instance, running this command on the connector we just loaded would give us:
```
  confluent local services connect connector status file-source

{
  "name": "file-source",
  "connector": {
    "state": "RUNNING",
    "worker_id": "192.168.10.1:8083"
  },
  "tasks": [
    {
      "state": "RUNNING",
      "id": 0,
      "worker_id": "192.168.10.1:8083"
    }
  ]
}
```
Soon after the connector starts, each of the three lines in our log file should be delivered to Kafka, having registered a schema with Schema Registry. One way to validate that the data is there is to use the console consumer in another console to inspect the contents of the topic:

```
kafka-avro-console-consumer --bootstrap-server localhost:9092 --topic connect-test --from-beginning
"log line 1"
"log line 2"
"log line 3"
```
WARNING : *Note that we use the kafka-avro-console-consumer because the data has been stored in Kafka using Avro format. This consumer uses the Avro converter that is bundled with Schema Registry in order to properly lookup the schema for the Avro data.*

### Write File Data with Connect

Now that we have written some data to a Kafka topic with Connect, let’s consume that data with a downstream process. In this section, we will load a sink connector to the worker in addition to the source that we started in the last section. The sink will write messages to a local file. This connector is also pre-defined in Confluent CLI confluent local commands under the name file-sink. Below is the connector’s configuration as it is stored in etc/kafka/connect-file-sink.properties:
```
# User defined name for the connector instance
name=file-sink
# Name of the connector class to be run
connector.class=FileStreamSink
# Max number of tasks to spawn for this connector instance
tasks.max=1
# Output file name relative to worker's current working directory
# This is the only property specific to the FileStreamSink connector
file=test.sink.txt
# Comma separate input topic list
topics=connect-test
```

Note that the configuration contains similar settings to the file source. A key difference is that multiple input topics are specified with topics whereas the file source allows for only one output topic specified with topic.

Now start the FileStreamSinkConnector. The sink connector will run within the same worker as the source connector, but each connector task will have its own dedicated thread.
```
  confluent local services connect connector load file-sink

{
  "name": "file-sink",
  "config": {
    "connector.class": "FileStreamSink",
    "tasks.max": "1",
    "file": "test.sink.txt",
    "topics": "connect-test",
    "name": "file-sink"
  },
  "tasks": []
}
```

To make sure the sink connector is up and running, use the Confluent CLI confluent local services connect connector status command to get the state of this specific connector:
```
  confluent local services connect connector status file-sink

{
  "name": "file-sink",
  "connector": {
    "state": "RUNNING",
    "worker_id": "192.168.10.1:8083"
  },
  "tasks": [
    {
      "state": "RUNNING",
      "id": 0,
      "worker_id": "192.168.10.1:8083"
    }
  ]
}
```
as well as the list of all loaded connectors:
```
  confluent local services connect connector status connectors

[
  "file-source",
  "file-sink"
]
```

Because of the rebalancing that happens between worker tasks every time a connector is loaded, a call to confluent local services connect connector status <connector-name> might not succeed immediately after a new connector is loaded. Once rebalancing completes, such calls will be able to return the actual status of a connector.

By opening the file test.sink.txt you should see the two log lines written to it by the sink connector.

Now, with both connectors running, we can see data flowing end-to-end in real time. To check this out, use another terminal to tail the output file:

```
tail -f test.sink.txt
```

and in a different terminal start appending additional lines to the text file:

for i in {4..1000}; do echo "log line $i"; done >> test.txt
You should see the lines being added to test.sink.txt. The new data was picked up by the source connector, written to Kafka, read by the sink connector from Kafka, and finally appended to the file.
```
"log line 1"
"log line 2"
"log line 3"
"log line 4"
"log line 5"
```
